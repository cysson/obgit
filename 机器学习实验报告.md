实验结果：经过训练，在val数据集上的AUROC为0.907.
在相同验证集上，视觉-语言模型（VLM）取得了 0.707 的图像级 AUROC，与 PatchCore 持平。值得注意的是，两种方法路径迥异：PatchCore 仅依赖 CNN 局部特征记忆，而 VLM 通过图文对齐机制将异常语义隐式编码到联合嵌入空间。结果持平一方面说明「异常」概念在语言模态中可被有效检索，另一方面也提示 VLM 尚未充分发挥其语义泛化优势——依旧停留在「判别」而非「理解」层面。未来若引入缺陷文本提示微调或图文对比蒸馏，有望利用语言先验突破纯视觉方法的天花板，实现小样本甚至零样本的跨品类异常检测。